{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial\n",
    "[Pytorch Tutorial学习笔记](https://www.yuque.com/hansonyun/cy91ib/ukvxkoi1s5ab6r5u) 这个是自己的学习教程，主要是为了记录自己的学习过程，方便以后查阅，也希望能够帮助到其他人。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tensor操作\n",
    "记录Tensor的一些操作示例，实践了才知道怎么使用。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor类型查看及检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Create a tensor of size (2, 3) from a normal distribution\n",
    "a = torch.randn(2, 3)\n",
    "\n",
    "b = torch.FloatTensor(2, 3)\n",
    "# Show tensor a type\n",
    "print(a.type())\n",
    "print(type(a)) # 这块有个疑问，为什么type(a)和a.type()的结果不一样？ 理论上来说type()也是输出子类的情况啊\n",
    "\n",
    "# Check tensor type\n",
    "print(isinstance(a, torch.Tensor))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor 数据从CPU转到GPU\n",
    "Tensor数据在CPU和GPU上时是不同的类型，在不同的设备上进行计算时，需要将数据转到相同的设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Create a tensor of size (2, 3) from a normal distribution\n",
    "a = torch.randn(2,3)\n",
    "\n",
    "# print current tensor device\n",
    "print(a.device)\n",
    "\n",
    "# transfer tensor to GPU 0\n",
    "if torch.cuda.is_available():\n",
    "    a = a.cuda()\n",
    "    print(a.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同维度的Tensor及其属性\n",
    "Tensor的维度是指Tensor的阶数，也就是Tensor的维度数目，比如一维Tensor的维度是1，二维Tensor的维度是2，以此类推。Tensor的维度属性是一个元组，元组的长度就是Tensor的维度，元组中的每个元素代表了Tensor在该维度上的长度。其维度可以通过Tensor的dim()方法查看，也可以通过Tensor的ndimension()方法查看。\n",
    "查看其属性可以通过Tensor的shape属性，也可以通过Tensor的size()方法。查看元素的个数可以通过Tensor的numel()方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 0维Tensor，标量，可以用于表示loss\n",
    "a = torch.tensor(1.0)\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "# 1维Tensor，向量，可以用于表示bias\n",
    "a = torch.tensor([1.0])\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "a = torch.tensor([1.0,2.0,3.0,4.0])\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "a = torch.FloatTensor(2)\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "# 2维Tensor，矩阵，可以用于表示weight\n",
    "a = torch.tensor([[1.0,2.0],[3.0,4.0]])\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "a = torch.FloatTensor(2,3)\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "# 3维Tensor，可以用于表示RNN的输入\n",
    "a = torch.tensor([[[1.0,2.0],[3.0,4.0]],[[5.0,6.0],[7.0,8.0]]])\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "a = torch.FloatTensor(2,3,4)\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "\n",
    "# 4维Tensor，可以用于表示CNN的输入\n",
    "a = torch.tensor([[[[1.0,2.0],[3.0,4.0]],[[5.0,6.0],[7.0,8.0]]],[[[9.0,10.0],[11.0,12.0]],[[13.0,14.0],[15.0,16.0]]]])\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "a = torch.FloatTensor(2,3,4,5)\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor的基本操作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的创建\n",
    "- 固定元素初始化创建(指定每个元素的值): 从list创建, torch.tensor();从numpy创建，torch.from_numpy()\n",
    "- 固定元素初始化创建(指定Tensor shape): torch.eye(), torch.zeros(), torch.zeros_like(), torch.ones(), torch.ones_like(), torch.full(), torch.full_like(), torch.range(), torch.arange(), torch.linspace(), torch.logspace()\n",
    "- 随机元素初始化创建(值随机选取): torch.empty(), torch.empty_like(), torch.Tensor(), torch.FloatTensor(), torch.IntTensor(), torch.DoubleTensor()\n",
    "- 随机元素初始化创建(指定值分布): torch.rand()，torch.rand_like(), torch.randint(), torch.randn()，torch.randn_like(), torch.normal(), torch.randperm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ------------------------------固定元素初始化创建(指定每个元素的值)--------------------------------\n",
    "# 从list创建Tensor\n",
    "a = torch.tensor(1.0, dtype=torch.float32); print(a)\n",
    "a = torch.tensor([1.0,2.0,3.0,4.0]); print(a)\n",
    "a = torch.tensor([[1.0,2.0],[3.0,4.0]]); print(a)\n",
    "\n",
    "# 从numpy创建Tensor\n",
    "a = torch.from_numpy(np.array([1.0,2.0,3.0,4.0])); print(a)\n",
    "\n",
    "#-----------------------------固定元素初始化创建(指定Tensor shape)--------------------------------\n",
    "# 从eye创建Tensor,只能创建2D Tensor\n",
    "a = torch.eye(3,4); print('eye', a)\n",
    "\n",
    "# 从zeros, zeros_like创建Tensor\n",
    "a = torch.zeros(3,4); print('zeros', a)\n",
    "a = torch.zeros_like(a); print('zeros_like', a)\n",
    "\n",
    "# 从ones, ones_like创建Tensor\n",
    "a = torch.ones(3,4); print('ones', a)\n",
    "a = torch.ones_like(a); print('ones_like', a)\n",
    "\n",
    "\n",
    "# 从full，full_like创建Tensor\n",
    "a = torch.full((3,4), 1.0); print('full', a)\n",
    "a = torch.full_like(a, 2.0); print('full_like', a)\n",
    "\n",
    "# 从range创建Tensor， range左闭右闭，且必须输入两个值; torch.range即将被弃用\n",
    "a = torch.range(1, 10); print('range', a)\n",
    "\n",
    "# 从arange创建Tensor，arange左闭右开\n",
    "a = torch.arange(1, 10, 2); print('arange', a)\n",
    "a = torch.arange(10); print('arange', a) # 从0开始, 默认步长为1\n",
    "a = torch.arange(10,0,-1); print('arange', a) # 从10开始，步长为-1\n",
    "\n",
    "# 从linespace创建Tensor，linespace左闭右闭\n",
    "a = torch.linspace(1, 10, 20); print('linspace', a)\n",
    "\n",
    "# 从logspace创建Tensor，logspace左闭右闭\n",
    "a = torch.logspace(1, 10, 20, 2); print('logspace', a) # 从[1,10]生成20个数，然后以2为底进行指数运算\n",
    "\n",
    "# ------------------------------------随机元素初始化创建(值随机选取)--------------------------------\n",
    "# 从empty创建Tensor\n",
    "a = torch.empty(3,4); print('empty', a)\n",
    "a = torch.empty_like(a); print('empty_like', a)\n",
    "\n",
    "# 从Tensor创建Tensor\n",
    "a = torch.Tensor(3,4); print('Tensor', a)\n",
    "a = torch.FloatTensor(3,4); print('FloatTensor', a)\n",
    "a = torch.IntTensor(3,4); print('IntTensor', a)\n",
    "a = torch.LongTensor(3,4); print('LongTensor', a)\n",
    "\n",
    "# ------------------------------------随机元素初始化创建(指定值分布)-------------------------------- \n",
    "# 从rand创建Tensor, rand为均匀分布\n",
    "a = torch.rand(3,4); print('rand', a)\n",
    "a = torch.rand_like(a); print('rand_like', a)\n",
    "\n",
    "# 从randint创建Tensor, randint为均匀分布\n",
    "a = torch.randint(0, 10, (3,4)); print('randint', a)\n",
    "\n",
    "# 从randn创建Tensor, randn为标准正态分布\n",
    "a = torch.randn(3,4); print('randn', a)\n",
    "a = torch.randn_like(a); print('randn_like', a)\n",
    "\n",
    "# 从normal创建Tensor, normal为正态分布\n",
    "a = torch.normal(mean=torch.full((10,), 0.0), std=torch.arange(1.0, 0.0, -0.1)); print('normal', a)\n",
    "\n",
    "# 从randperm创建Tensor, randperm为随机排列，用于生成随机不重复整数序列\n",
    "a = torch.randperm(10); print('randperm', a)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor索引及切片\n",
    "Tensor的索引及切片操作与numpy的操作类似，但是Tensor的索引及切片操作是在原Tensor上进行的，而numpy的索引及切片操作是在原数据的拷贝上进行的。也就是说，索引得到的是原数据的引用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(4,3,28,28)\n",
    "print(\"a\",a)\n",
    "\n",
    "# 验证索引是原数据的引用\n",
    "a[0][0][0][0] = 2023.0\n",
    "print(\"change a\", a)\n",
    "\n",
    "# 索引单个位置\n",
    "a_0 = a[0]; print('a_0', a_0.shape)\n",
    "a_0_0 = a[0][0]; print('a_0_0', a_0_0.shape) # 或a[0,0]\n",
    "a_0_0_0 = a[0][0][0]; print('a_0_0_0', a_0_0_0.shape) # 或a[0,0,0]\n",
    "a_0_0_0_0 = a[0][0][0][0]; print('a_0_0_0_0', a_0_0_0_0, a_0_0_0_0.shape) # 或a[0,0,0,0]\n",
    "\n",
    "# 索引前N个或后N个\n",
    "a_2 = a[:2]; print('a_2', a_2.shape)\n",
    "a_2_1 = a[:2,:1]; print('a_2_1', a_2_1.shape)\n",
    "a_2_1_3 = a[:2,:1,:3]; print('a_2_1_3', a_2_1_3.shape) # 或a[:2,:1,:3]\n",
    "\n",
    "# 根据步长索引\n",
    "a_s_2 = a[::2]; print('a_s_2', a_s_2.shape)\n",
    "a_s_2 = a[:,::2,1:10:2,:]; print('a_s_2', a_s_2.shape)\n",
    "\n",
    "# 根据index_select索引,维度，索引具体位置\n",
    "a_0 = a.index_select(0, torch.tensor([0,1])); print('a_0', a_0.shape)\n",
    "\n",
    "# 使用...索引，...表示任意多维, 会根据前后维度自动补全\n",
    "a_0 = a[0,...]; print('a_0', a_0.shape)\n",
    "a_0 = a[0,...,1]; print('a_0', a_0.shape)\n",
    "a_0 = a[0,...,1:10:2]; print('a_0', a_0.shape)\n",
    "\n",
    "# 使用masked_select索引, 默认会将Tensor转换为1D Tensor\n",
    "mask = a.ge(0.5); print('mask', mask)\n",
    "a_0 = a.masked_select(mask); print('a_0', a_0.shape)\n",
    "\n",
    "# 使用take索引, 默认会将Tensor转换为1D Tensor, 然后在根据传入的1D Tensor进行索引\n",
    "a_0 = a.take(torch.tensor([0,1,2,3,4,5,6,7,8,9])); print('a_0', a_0.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的维度变换\n",
    "- view/reshape: view和reshape的作用是将Tensor的维度变换为指定的维度，但是view只能用于contiguous Tensor，而reshape可以用于任意Tensor。这里需要注意view和reshape后的内容要有意义。\n",
    "- squeeze/unsqueeze: squeeze和unsqueeze的作用是一样的，都是在Tensor的指定维度上增加或者减少一个维度。unsqueeze有个口诀，传入正数在之前插入，传入负数在之后插入。\n",
    "- transpose/t/permute: transpose和t的作用是一样的，都是将Tensor的两个维度进行交换，permute可以对Tensor的多个维度进行交换。\n",
    "- expand/repeat: expand和repeat的作用是一样的，都是将Tensor的指定维度进行复制; expand要求Tensor在指定维度上的长度为1，而repeat没有这个要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view操作进行Tensor维度变换, 更改前后元素数量要一致\n",
    "a = torch.randn(4,3,28,28)\n",
    "a_0 = a.view(4,3,28*28); print('a_0', a_0.shape) # 合并最后两个维度，从图像上来理解的话，变成了4张图像，每张图像有3个通道，每个通道有784个像素点\n",
    "a_0 = a.view(4,3*28,28); print('a_0', a_0.shape) # 合并中间两个维度，从图像上来理解的话，变成了4张图像，每张图像有84行，每行有28个像素点\n",
    "a_0 = a.view(4*3,28,28); print('a_0', a_0.shape) # 合并前两个维度，从图像上来理解的话，变成了12张图像，每张图像有28行，每行有28个像素点\n",
    "\n",
    "# reshape操作进行Tensor维度变换, 更改前后元素数量要一致\n",
    "a_1 = a.reshape(4,3,28*28); print('a_1', a_1.shape) # 合并最后两个维度，从图像上来理解的话，变成了4张图像，每张图像有3个通道，每个通道有784个像素点\n",
    "a_1 = a.reshape(4,3*28,28); print('a_1', a_1.shape) # 合并中间两个维度，从图像上来理解的话，变成了4张图像，每张图像有84行，每行有28个像素点\n",
    "a_1 = a.reshape(4*3,28,28); print('a_1', a_1.shape) # 合并前两个维度，从图像上来理解的话，变成了12张图像，每张图像有28行，每行有28个像素点\n",
    "\n",
    "# squeeze操作进行Tensor维度变换, 去除维度为1的维度\n",
    "a = torch.randn(1,3,1,1)\n",
    "a_2 = a.squeeze(); print('a_2', a_2.shape)\n",
    "a_2 = a.squeeze(0); print('a_2', a_2.shape)\n",
    "a_2 = a.squeeze(2); print('a_2', a_2.shape)\n",
    "\n",
    "# unsqueeze操作进行Tensor维度变换, 在指定位置增加维度为1的维度\n",
    "a = torch.randn(3,28,28)\n",
    "a_3 = a.unsqueeze(0); print('a_3', a_3.shape)\n",
    "a_3 = a.unsqueeze(1); print('a_3', a_3.shape)\n",
    "a_3 = a.unsqueeze(-1); print('a_3', a_3.shape)\n",
    "\n",
    "# t操作进行Tensor维度变换, 交换维度; 只能用于2D Tensor\n",
    "a = torch.randn(3,4)\n",
    "a_4 = a.t(); print('a_4', a_4.shape)\n",
    "\n",
    "# transpose操作进行Tensor维度变换, 交换维度; 可以用于多维Tensor\n",
    "a = torch.randn(3,4,5)\n",
    "a_5 = a.transpose(1,2); print('a_5', a_5.shape)\n",
    "a_5 = a.transpose(0,2); print('a_5', a_5.shape)\n",
    "a_5 = a.transpose(0,1); print('a_5', a_5.shape)\n",
    "\n",
    "# permute操作进行Tensor维度变换, 交换维度; 可以用于多维Tensor\n",
    "a = torch.randn(3,4,5)\n",
    "a_6 = a.permute(1,2,0); print('a_6', a_6.shape)\n",
    "a_6 = a.permute(2,0,1); print('a_6', a_6.shape)\n",
    "a_6 = a.permute(0,2,1); print('a_6', a_6.shape)\n",
    "\n",
    "# expand操作进行Tensor维度变换, 如果待扩展维度为1，则扩展为指定维度；如果待扩展维度不为1且与原维度不一致，则报错\n",
    "a = torch.randn(1,4,5)\n",
    "a_7 = a.expand(3,4,5); print('a_7', a_7.shape)\n",
    "\n",
    "# repeat操作进行Tensor维度变换, 传入参数为每个维度的重复次数\n",
    "a = torch.randn(3,4,5)\n",
    "a_8 = a.repeat(2,1,1); print('a_8', a_8.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Broadcast自动扩展\n",
    "tensor broadcast是指在进行运算时，如果两个tensor的shape不一样，会自动扩展成一样的shape，然后再进行运算。tensor broadcast的规则如下：\n",
    "- 对两个tensor的shape从后往前进行比较，如果两个tensor的shape在某个维度上相同或者其中一个tensor在该维度上的长度为1，则这两个tensor在该维度上是兼容的，可以进行broadcast。\n",
    "- 如果两个tensor在某个维度上都不为1，且两个tensor在该维度上的长度不相等，则这两个tensor在该维度上是不兼容的，不能进行broadcast。\n",
    "- 满足第一个条件的情况下，如果两个tensor的维度数目不同，则在较小的tensor的shape前面补1，使得两个tensor的维度数目相同，然后再进行比较。\n",
    "  \n",
    "为什么要使用Broadcast？ 一是有实际需求，二是可以减少内存的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast操作进行Tensor自动扩展\n",
    "a = torch.randn(3,4,5)\n",
    "b = torch.randn(4,5)\n",
    "c = a + b; print('c', c.shape)\n",
    "\n",
    "a = torch.randn(3,1)\n",
    "b = torch.randn(1,3)\n",
    "c = a + b; print('c', c.shape)\n",
    "\n",
    "a = torch.randn(3,1,5)\n",
    "b = torch.tensor([0.5])\n",
    "c = a * b; print('c', c.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor拼接及分割\n",
    "- cat: cat是concatenate的缩写，指的是将多个Tensor在指定维度上进行拼接;要求除了指定维度外，其他维度的长度必须一致。\n",
    "- stack: stack是将多个Tensor在指定维度上进行堆叠，堆叠后的Tensor的维度比原来的Tensor的维度多1;要求待堆叠的Tensor的shape必须一致。\n",
    "- split: split是将Tensor在指定维度上进行分割\n",
    "- chunk: chunk是将Tensor在指定维度上进行分割；chunk是分块的意思，从官网上的意思看，如果可以整除，则每个块的长度相同，如果不能整除，则最后一个块的长度可能会不同（是不是这里隐含着最后一个块的大小要小于其他块）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat操作进行Tensor拼接, 拼接的维度必须一致\n",
    "a = torch.randn(3,4)\n",
    "b = torch.randn(3,5)\n",
    "c = torch.cat([a,b], dim=1); print('c', c.shape)\n",
    "\n",
    "# stack操作进行Tensor拼接, 拼接的维度必须一致；dim指定拼接的维度\n",
    "a = torch.randn(3,4)\n",
    "b = torch.randn(3,4)\n",
    "c = torch.stack([a,b], dim=0); print('c', c.shape)\n",
    "\n",
    "# split操作进行Tensor拆分\n",
    "a = torch.randn(3,4)\n",
    "b, c = a.split([1,2], dim=0); print('b', b.shape,'c', c.shape)\n",
    "b, c, d = a.split([1,1,2], dim=1); print('b', b.shape,'c', c.shape, 'd', d.shape)\n",
    "\n",
    "# chunk操作进行Tensor拆分\n",
    "a = torch.randn(3,5)\n",
    "b, c = a.chunk(2, dim=0); print('b', b.shape,'c', c.shape)\n",
    "b, c, d = a.chunk(3, dim=1); print('b', b.shape,'c', c.shape, 'd', d.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的基本运算\n",
    "- Tensor逐元素加减乘除: +, -, *, /, add, sub, mul, div\n",
    "- Tensor乘法： @, matmul, mm(只适合二维Tensor)\n",
    "- Tensor逐元素n次方: pow(**), sqrt, rsqrt\n",
    "- Tensor逐元素取对数: log, log2, log10, log1p\n",
    "- Tensor逐元素取指数: exp, exp2\n",
    "- Tensor逐元素取近似值: ceil, floor, round, trunc, frac\n",
    "- Tensor逐元素取余: fmod, remainder；两种只是在trunc_mode上有些不同\n",
    "- Tensor逐元素取绝对值: abs, absolute\n",
    "- Tensor逐元素取符号: sign\n",
    "- Tensor逐元素取上下限: clamp, clamp_max, clamp_min, clip\n",
    "- Tensor逐元素取反: neg\n",
    "- Tensor逐元素比较: >, >=, <, <=, ==, !=, gt, ge, lt, le, eq, ne，isclose\n",
    "- Tensor之间比较: allclose, equal\n",
    "- Tensor逐元素逻辑运算: &, |, ^, ~, logical_and, logical_or, logical_xor, logical_not\n",
    "- Tensor逐元素取倒数: reciprocal\n",
    "- Tensor逐元素取三角函数: sin, cos, tan, asin, acos, atan, atan2, sinh, cosh, tanh, asinh, acosh, atanh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Tensor逐元素加减乘除，要求两个Tensor的shape一致\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "b = torch.randn(3,4); print('b', b)\n",
    "c = a + b; print('+', c)\n",
    "c = a.add(b); print('add', c)\n",
    "c = a - b; print('-', c)\n",
    "c = a.sub(b); print('sub', c)\n",
    "c = a * b; print('*', c)\n",
    "c = a.mul(b); print('mul', c)\n",
    "c = a / b; print('/', c)\n",
    "c = a.div(b); print('div', c)\n",
    "\n",
    "# Tensor 二维mm乘法，要求符合矩阵乘法规则\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "b = torch.randn(4,5); print('b', b)\n",
    "c = torch.mm(a,b); print('mm', c)\n",
    "\n",
    "# Tensor matmul乘法，要求符合矩阵乘法规则, 除最后两维外，其他维度必须一致\n",
    "a = torch.randn(3,4,5); print('a', a)\n",
    "b = torch.randn(3,5,6); print('b', b)\n",
    "c = torch.matmul(a,b); print('matmul', c)\n",
    "c = a@b; print('@', c)\n",
    "\n",
    "# Tensor 逐元素n次方\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.pow(2); print('pow', c)\n",
    "c = a**2; print('**', c)\n",
    "c = a.sqrt(); print('sqrt', c)      # sqrt 为平方根\n",
    "c = a.rsqrt(); print('rsqrt', c)    # rsqrt 为平方根倒数(reciprocal of the square root)\n",
    "\n",
    "# Tensor 逐元素取对数\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.log(); print('log', c)\n",
    "c = a.log2(); print('log2', c)\n",
    "c = a.log10(); print('log10', c)\n",
    "c = a.log1p(); print('log1p', c)    # log1p 为log(1+x)\n",
    "\n",
    "# Tensor 逐元素取指数\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.exp(); print('exp', c)\n",
    "c = a.exp2(); print('exp2', c)    # exp2 为2的指数\n",
    "\n",
    "# Tensor 逐元素近似\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.ceil(); print('ceil', c)    # ceil 为向上取整\n",
    "c = a.floor(); print('floor', c)  # floor 为向下取整\n",
    "c = a.round(); print('round', c)  # round 为四舍五入\n",
    "c = a.trunc(); print('trunc', c)  # trunc 为截断取整\n",
    "c = a.frac(); print('frac', c)    # frac 为取小数部分\n",
    "\n",
    "# Tensor 逐元素取余\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.remainder(1); print('remainder', c)    # remainder: torch.remainder(a, b) == a - a.div(b, rounding_mode=\"floor\") * b\n",
    "c = a.fmod(1); print('fmod', c)              # fmod: torch.fmod(a, b) == a - a.div(b, rounding_mode=\"trunc\") * b\n",
    "\n",
    "# Tensor 逐元素取绝对值\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.abs(); print('abs', c)\n",
    "c = a.absolute(); print('absolute', c)\n",
    "\n",
    "# Tensor 逐元素取符号\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.sign(); print('sign', c)\n",
    "\n",
    "# Tensor 逐元素取上下限\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.clamp(-0.1, 0.1); print('clamp', c)\n",
    "c = a.clamp_max(0.1); print('clamp_max', c)\n",
    "c = a.clamp_min(-0.1); print('clamp_min', c)\n",
    "c = a.clip(-0.1, 0.1); print('clip', c)\n",
    "\n",
    "# Tensor 逐元素取反\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.neg(); print('neg', c)\n",
    "\n",
    "# Tensor 逐元素比较\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "b = torch.randn(3,4); print('b', b)\n",
    "c = a > b; print('>', c)\n",
    "c = a >= b; print('>=', c)\n",
    "c = a < b; print('<', c)\n",
    "c = a <= b; print('<=', c)\n",
    "c = a == b; print('==', c)\n",
    "c = a != b; print('!=', c)\n",
    "c = a.isclose(b); print('isclose', c)\n",
    "\n",
    "# Tensor 比较\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "b = torch.randn(3,4); print('b', b)\n",
    "c = torch.equal(a, b); print('equal', c)\n",
    "c = torch.allclose(a, b); print('allclose', c)\n",
    "\n",
    "# Tensor 逐元素逻辑运算\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "b = torch.randn(3,4); print('b', b)\n",
    "c = torch.logical_and(a>0, b>0); print('logical_and', c)\n",
    "c = torch.logical_or(a>0, b>0); print('logical_or', c)\n",
    "c = torch.logical_xor(a>0, b>0); print('logical_xor', c)\n",
    "c = torch.logical_not(a>0); print('logical_not', c)\n",
    "\n",
    "# Tensor 逐元素取倒数\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.reciprocal(); print('reciprocal', c)\n",
    "\n",
    "# Tensor 逐元素取三角函数\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.sin(); print('sin', c)\n",
    "c = a.cos(); print('cos', c)\n",
    "c = a.tan(); print('tan', c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的属性及统计信息\n",
    "- Tensor的最大最小值及位置: max, min，argmax, argmin\n",
    "- Tensor的均值及方差: mean, std\n",
    "- Tensor指定维度 的累加和累乘: sum, prod; \n",
    "- Tensor指定维度的累加和累乘: cumsum, cumprod; 会保持原Tensor的维度，只是在指定维度上进行累加或累乘\n",
    "- Tensor的范数: norm\n",
    "- Tensor的距离: dist\n",
    "- Tensor的非零元素: nonzero\n",
    "- Tensor的k统计属性: topk, kthvalue\n",
    "- Tensor的唯一元素: unique; 返回剔除重复元素后的Tensor\n",
    "- Tensor的元素属性判断: isfinite, isinf, isnan, isneginf, isposinf\n",
    "- Tensor的统计: bincount, count_nonzero, histogram, histogramdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor([[ 0.0161,  1.8801,  0.3771,  0.5195],\n",
      "        [-0.8812,  0.6425,  0.2978,  0.0712],\n",
      "        [-0.8885, -1.6109, -2.2392, -0.7111]])\n",
      "isfinite tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n",
      "isinf tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "isnan tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "isposinf tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "isneginf tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "# # Tensor的最大值最小值及其索引，可以使用dim参数指定维度以及keepdim参数保持维度不变\n",
    "# a = torch.randn(3,4); print('a', a)\n",
    "# c = a.max(dim=1,keepdim=True); print('max', c)\n",
    "# c = a.min(); print('min', c)\n",
    "# c = a.argmax(dim=1,keepdim=True); print('argmax', c)\n",
    "# c = a.argmin(); print('argmin', c)\n",
    "\n",
    "# # Tensor的均值及方差, 可以使用dim参数指定维度以及keepdim参数保持维度不变\n",
    "# a = torch.randn(3,4); print('a', a)\n",
    "# c = a.mean(dim=1,keepdim=True); print('mean', c)\n",
    "# c = a.std(); print('std', c)\n",
    "\n",
    "# # Tensor所有要素的累加和累积\n",
    "# a = torch.randn(3,4); print('a', a)\n",
    "# c = a.sum(); print('sum', c)\n",
    "# c = a.prod(); print('prod', c)\n",
    "\n",
    "# # Tensor所有要素的累加和累积，保持维度不变\n",
    "# a = torch.randn(3,4); print('a', a)\n",
    "# c = a.cumsum(dim=1); print('cumsum', c)\n",
    "# c = a.cumprod(dim=1); print('cumprod', c)\n",
    "\n",
    "# # Tensor的范数，范数的计算公式为：norm = (x1^p + x2^p + ... + xn^p)^(1/p)\n",
    "# a = torch.randn(3,4); print('a', a)\n",
    "# c = a.norm(1); print('norm-1', c)\n",
    "# c = a.norm(2); print('norm-2', c)\n",
    "\n",
    "# # Tensor的距离，距离的计算公式为：dist = (x1-y1)^p + (x2-y2)^p + ... + (xn-yn)^p)^(1/p)\n",
    "# a = torch.randn(3,4); print('a', a)\n",
    "# b = torch.randn(3,4); print('b', b)\n",
    "# c = torch.dist(a, b, 1); print('dist-1', c)\n",
    "# c = torch.dist(a, b, 2); print('dist-2', c)\n",
    "\n",
    "# # Tensor的非0元素, 返回的是非0元素的索引\n",
    "# a = torch.randn(3,4); print('a', a)\n",
    "# c = a.nonzero(); print('nonzero', c)\n",
    "\n",
    "# # Tensor的k属性统计\n",
    "# a = torch.randn(3,4); print('a', a)\n",
    "# c = a.kthvalue(2,dim=0); print('kthvalue', c)\n",
    "# c = a.topk(2,dim=0); print('topk', c)\n",
    "\n",
    "# # Tensor的unique属性统计\n",
    "# a = torch.tensor([1,2,3,4,5,6,7,8,9,10,1,2,3,4,5]); print('a', a)\n",
    "# c = a.unique(); print('unique', c)\n",
    "\n",
    "# # Tensor逐元素判断特性\n",
    "# a = torch.randn(3,4); print('a', a)\n",
    "# c = a.isfinite(); print('isfinite', c)\n",
    "# c = a.isinf(); print('isinf', c)\n",
    "# c = a.isnan(); print('isnan', c)\n",
    "# c = a.isposinf(); print('isposinf', c) # 正无穷\n",
    "# c = a.isneginf(); print('isneginf', c) # 负无穷\n",
    "\n",
    "# Tensor的统计信息\n",
    "a = torch.randn(3,4); print('a', a)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的一些高级操作\n",
    "- Tensor的where操作\n",
    "- Tensor的gather操作"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_tw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
